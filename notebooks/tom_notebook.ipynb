{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "157731de-407c-4db2-86b3-c4c9ebcc4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822efa5-8398-418f-b326-26a98c7d8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_dims):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        #*input_shape = expecting a tuple and will unpack\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), \n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), \n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        \n",
    "        #Done flags from the environemnt,so the agent recieves no future rewards\n",
    "        #once it encounters the terminal state (0) as the game is over\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "\n",
    "    #Function to store our tuple in our agents memory\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "\n",
    "        #position of first unoccupied memory\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        #set index value to what we pass in\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_       \n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "\n",
    "        #increment our memory counter by one\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    #As we have a finite subset of memories, we dont want to initialise zeros,\n",
    "    #you only want to sample valid data, as we wont learn anything from zeros.\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]        \n",
    "​\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba9d68-1698-4aa2-918b-3e4b519e43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(lr, neurons_layer1, neurons_layer2, neurons_layer3, n_actions, input_dims):\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Dense(fc1_dims, activation='relu'),\n",
    "            keras.layers.Dense(fc2_dims, activation='relu'),\n",
    "            keras.layers.Dense(n_actions, activation=None)])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf0d44fd-1bc2-4207-afb3-2bcf4b43c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class: \n",
    "\n",
    "# Where the bulk of the functionality lives:\n",
    "## The agent class has a DQN, memory, functionality for choosing actions, storing memories and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab7ca-3149-405b-96f7-e7b3d82938e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size, \n",
    "                  input_dims, epsilon_dec=0.9, epsilon_end=0.01, \n",
    "                  mem_size=10000, replace_target=100, fname='rl_control_model.h5'):\n",
    "\n",
    "    #Dont train both networks\n",
    "    #Only train the target network that you use to choose actions, \n",
    "    #replace the weights of the target network every 100 episodes.\n",
    "        \n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target = replace_target\n",
    "        self.model_file = fname\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "        self.q_eval = build_dqn(lr, 8, 8, 8, n_actions, input_dims)\n",
    "        self.q_targ = build_dqn(lr, 8, 8, 8 n_actions, input_dims)\n",
    "\n",
    "\n",
    "    #Stores the the ​state-action-reward\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    #Choose an action, i.e. a policy, chooses an action based on its current state\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = np.array([observation])\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "\n",
    "        #When do we perform learning? Once the max_size is reach and memory is full?\n",
    "        #If the number of memories saved is less than the batch size you will end up\n",
    "        #sampling a single memory batch size times (single memory 64 times), you will\n",
    "        #end up sampling the same batch 64 times - not good for training.\n",
    "\n",
    "        #Double Q = q-value of a q-value\n",
    "        \n",
    "        if self.memory.mem_cntr > self.batch_size:\n",
    "​\n",
    "            state, actions, rewards, state_, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "            action_values = np.array(self.action_space, dtype=np.int8)\n",
    "            action_indices = np.dot(action, action_values)\n",
    "            \n",
    "            q_next = self.q_target.predict(state_)\n",
    "            q_eval = self.q_eval.predict(state_)\n",
    "\n",
    "            q_pred = self.q_eval.predict(state)\n",
    "     \n",
    "            max_actions = np.argmax(q_eval, axis=1)\n",
    "\n",
    "            q_target = np.copy(q_prednp)\n",
    "​\n",
    "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "            \n",
    "            q_target[batch_index, actions] = rewards + \\\n",
    "                self.gamma * q_next[batch_index, max_actions.astype(int)]*done\n",
    "            \n",
    "            losses = self.q_eval.fit(state, q_target, verbose=0)\n",
    "\n",
    "            if self.memory.mem_cntr % self.replace_target == 0:                \n",
    "                self.update_network()\n",
    "                \n",
    "            return losses\n",
    "​\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > \\\n",
    "            self.epsilon_min else self.epsilon_min \n",
    "        \n",
    "    def update_network(self):  \n",
    "            self.q_targ.set_weights(self.q_eval.get_weights()) \n",
    "        \n",
    "   \n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file)\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file) \n",
    "        \n",
    "        if self.epsilon <= self.epsilon_min:\n",
    "            self.update_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748e0cd-1312-43db-985b-ddc092b2f380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
